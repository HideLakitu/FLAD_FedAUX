"""简单版Server自监督预训练模块使用 MLP作为特征提取器进行无监督预训练"""import torchimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoader, TensorDatasetimport numpy as npclass SimpleMLPEncoder(nn.Module):    def __init__(self, input_dim, hidden_dim=128):        super(SimpleMLPEncoder, self).__init__()        self.encoder = nn.Sequential(            nn.Flatten(),  # 把 (time_window, features) 展平成一维            nn.Linear(input_dim, hidden_dim),            nn.ReLU(),            nn.Linear(hidden_dim, input_dim)  # 重建原始输入        )    def forward(self, x):        return self.encoder(x)def server_self_supervised_pretrain(daux_npy_path, output_model_path, batch_size=128, epochs=20, learning_rate=1e-3):    # 加载辅助数据    X = np.load(daux_npy_path)  # shape: (N, time_window, features)    N, T, F = X.shape    input_dim = T * F    # 转为Torch Tensor    X_tensor = torch.tensor(X, dtype=torch.float32)    # 构建数据集和DataLoader    dataset = TensorDataset(X_tensor)    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)    # 初始化模型    model = SimpleMLPEncoder(input_dim)    optimizer = optim.Adam(model.parameters(), lr=learning_rate)    loss_fn = nn.MSELoss()    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    model = model.to(device)    # 训练过程    model.train()    for epoch in range(epochs):        epoch_loss = 0.0        for batch in dataloader:            batch_x = batch[0].to(device)            optimizer.zero_grad()            outputs = model(batch_x)            loss = loss_fn(outputs, batch_x.view(batch_x.size(0), -1))  # 与输入自己比            loss.backward()            optimizer.step()            epoch_loss += loss.item() * batch_x.size(0)        avg_loss = epoch_loss / len(dataset)        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}")    # 保存训练好的Encoder权重    torch.save(model.state_dict(), output_model_path)    print(f"✅ Encoder预训练完成，已保存至 {output_model_path}")    return model# 示例使用if __name__ == "__main__":    Daux_path = "./FEDAUX/aux_data/aux2.npy"    output_path = "./FEDAUX/pretain_server/s2.pth"    server_self_supervised_pretrain(Daux_path, output_path)